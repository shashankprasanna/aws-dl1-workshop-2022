[
{
	"uri": "/1_getting_started/login_aws_account.html",
	"title": "1.1 Login to your temporary workshop AWS Account",
	"tags": [],
	"description": "",
	"content": " Get your temporary AWS account Click on the link at the bottom of the browser as show below.\nIf using Windows Save file to a convenient location such as your Desktop\nIf using MAC/Ubuntu  Save file to a convenient location such as Desktop You\u0026rsquo;ll need to perform update the file\u0026rsquo;s permissions by running the code below the screenshot in your terminal  Note: advanced users may optionally save it at ~/.ssh/ and make sure to use the correct path when using it to connect to your DL1 EC2 instance\n cd Desktop chmod 400 labsuser.pem  Note: If you saved the labsuser.pem at a different location, make sure to use the correct path.\n "
},
{
	"uri": "/2_getting_started_dl1/download_habana_models.html",
	"title": "2.1 Download habana Models",
	"tags": [],
	"description": "",
	"content": " Training on Habana from the Model-References GitHub repository Habana has a Model-References GitHub Page with a list of reference models that are optimized to run on Gaudi There are models for TensorFlow and PyTorch covering Computer Vision and NLP usages\nFocus will be on TensorFlow ResNet50 Keras and BERT examples in this workshop. In general, when using the Full DLAMI, the user will need to to the following to run Habana models:\n Clone the GitHub Repository Set the PYTHON ENV Variable export PYTHON=/usr/bin/python3.7 for Ubuntu18.04 Download appropriate dataset Select Hyperparameters an options Train and post processing  "
},
{
	"uri": "/3_migrate_models_dl1/simple_model.html",
	"title": "3.1 Simple Model Example",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/appendix/docs.html",
	"title": "Documentation resources",
	"tags": [],
	"description": "",
	"content": " 1. SageMaker SDK API guide https://sagemaker.readthedocs.io/en/stable/\n2. SageMaker Sample Notebooks on GitHub https://github.com/awslabs/amazon-sagemaker-examples\n3. SageMaker Developer Guide https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html\n4. SageMaker API Reference https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_Operations_Amazon_SageMaker_Service.html\n"
},
{
	"uri": "/",
	"title": "Getting Started with Amazon EC2 DL1 instances",
	"tags": [],
	"description": "",
	"content": " Getting Started with Amazon EC2 DL1 instances Presenters    Shashank Prasanna, Sr. Developer Advocate, AI/ML Greg Serochi Intel     Twitter: @shshnkp\nLinkedIn: linkedin.com/in/shashankprasanna\nBlog: medium.com/@shashankprasanna     Workshop Overview Workshop duration: 4 hours\nAbstract: In this workshop you learn how to use the new AWS DL1 instance featuring the Habana Gaudi HPU. This 4 hour session will introduce users to the new instance and show the step-by-step to starting an instance, running Habana’s default models and taking a public model to the instance. You will also learn how to author your own models in TensorFlow and train them with Habana Gaudi HPU. Will include hands-on lab with Habana and AWS engineers showing how to execute, profile models in Amazon EC2.\nAgenda    Topics Duration     Workshop overview and DL1 setup 30 mins   Getting started 45 mins   Deep Learning Training with Habana Models 45 mins   Deep Learning with Custom Models 60 mins   Wrap Up 20 mins    "
},
{
	"uri": "/0_introduction.html",
	"title": "Workshop Interface Overview",
	"tags": [],
	"description": "",
	"content": "Watch this short video at anytime to familiarize yourself with the workshop interface.   "
},
{
	"uri": "/1_getting_started.html",
	"title": "1. Getting started",
	"tags": [],
	"description": "",
	"content": "For this workshop you\u0026rsquo;ll get access to a temporary AWS Account already pre-configured to run Amazon EC2 DL1 instances. Follow the steps in this section to login to your AWS Account and download the workshop material.\n"
},
{
	"uri": "/1_getting_started/launch_dl1_instance.html",
	"title": "1.2 Launch an Amazon EC2 DL1 instance",
	"tags": [],
	"description": "",
	"content": " Get your temporary AWS account "
},
{
	"uri": "/2_getting_started_dl1/rs50.html",
	"title": "2.2 ResNet50 Keras Notebook",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/3_migrate_models_dl1/hw_and_sw.html",
	"title": "3.2 Habana Hardware and Software",
	"tags": [],
	"description": "",
	"content": " Hardware Gaudi is designed from the ground up for accelerating DL training workloads. Its heterogeneous architecture comprises a cluster of fully programmable Tensor Processing Cores (TPC) along with its associated development tools and libraries, and a configurable Matrix Math engine.\nThe TPC core is a VLIW SIMD processor with instruction set and hardware tailored to serve training workloads efficiently. It is programmable, providing the user with maximum flexibility to innovate, coupled with many workload-oriented features, such as:\n GEMM operation acceleration Tensor addressing Latency hiding capabilities Random number generation Advanced implementation of special functions  The TPC core natively supports the following data types: FP32, BF16, INT32, INT16, INT8, UINT32, UINT16 and UINT8.\nThe Gaudi memory architecture includes on-die SRAM and local memories in each TPC. In addition, the chip package integrates four HBM devices, providing 32 GB of capacity and 1 TB/s bandwidth. The PCIe interface provides a host interface and supports both generation 3.0 and 4.0 modes.\nGaudi is the first DL training processor that has integrated RDMA over Converged Ethernet (RoCE v2) engines on-chip. With bi-directional throughput of up to 2 TB/s, these engines play a critical role in the inter-processor communication needed during the training process. This native integration of RoCE allows customers to use the same scaling technology, both inside the server and rack (scale-up), as well as to scale across racks (scale-out). These can be connected directly between Gaudi processors, or through any number of standard Ethernet switches.\nSoftware Designed to facilitate high-performance DL training on Habana’s Gaudi accelerators, SynapseAI Software Suite enables efficient mapping of neural network topologies onto Gaudi hardware. The software suite includes Habana’s graph compiler and runtime, TPC kernel library, firmware and drivers, and developer tools such as the TPC SDK for custom kernel development and SynapseAI Profiler. SynapseAI is integrated with popular frameworks, TensorFlow and PyTorch, and performance-optimized for Gaudi\nGraph Compiler and Runtime The SynapseAI graph compiler generates optimized binary code that implements the given model topology on Gaudi. It performs operator fusion, data layout management, parallelization, pipelining and memory management, as well as graph-level optimizations. The graph compiler uses the rich TPC kernel library which contains a wide variety of operations (for example, elementwise, non-linear, non-GEMM operators). Kernels for training have two implementations, forward and backward.\nGiven the heterogeneous nature of Gaudi hardware (Matrix Math engine, TPC and DMA), the SynapseAI graph compiler enables effective utilization through parallel and pipelined execution of framework graphs. SynapseAI uses stream architecture to manage concurrent execution of asynchronous tasks. It includes a multi-stream execution environment supporting Gaudi’s unique combination of compute and networking as well as exposing a multi-stream architecture to the framework. Streams of different types — compute, networking and DMA — are synchronized with one another at high performance and with low run-time overheads.\nDL Framework Integration Popular DL frameworks such as TensorFlow and PyTorch are integrated with SynapseAI and optimized for Gaudi. SynapseAI does this under the hood, so customers still enjoy the same abstraction in TensorFlow and PyTorch that they are accustomed to today. The SynapseAI TensorFlow/PyTorch bridge identifies the subset of the framework’s computation graph that can be accelerated by Gaudi. These subgraphs are executed optimally on Gaudi. For performance optimization, the compilation recipe is cached for future use. Operators that are not supported by Gaudi are executed on the CPU.\nHabana Communication Libraries The Habana Communication Library (HCL) enables efficient scale-up communication between Gaudi processors within a single node and scale-out across nodes for distributed training, leveraging Gaudi’s high performance RDMA communication capabilities. It has an MPI look-and-feel and supports point-to- point operations (for example, Write, Send) and collective operations (for example, AllReduce, AlltoAll) that are performance optimized for Gaudi. See Habana Communication Library (HCL) API Reference for further details.\nThe SynapseAI suite main interface is the Habana Collective Communications Library (HCCL) which is Habana’s implementation of standard collective communication routines with NCCL-compatible API. HCL uses Gaudi integrated NICs for both scale-up and scale-out. HCCL allows users to enable Gaudi integrated NIC for scale-up and host NIC for scale-out.\nTPC Programming The SynapseAI TPC SDK includes an LLVM-based TPC-C compiler, a simulator and debugger. These tools facilitate the development of custom TPC kernels. This SDK is used by Habana to build the high-performance kernels we provide to users. You can thereby develop customized deep learning models and algorithms on Gaudi to innovate and optimize to your unique requirements.\n"
},
{
	"uri": "/1_getting_started/launch_jupyter_dl1.html",
	"title": "1.3 Launch Jupyterlab server",
	"tags": [],
	"description": "",
	"content": " Launch Jupyterlab server on your DL1 instance SSH to your DL1 instance cd cd Desktop ssh -i labsuser.pem ubuntu@\u0026lt;PASTE_IP_ADDRESS\u0026gt;  Note: Make sure you give the full path to the labsuser.pem file or navigate to the directory where you saved the file before running SSH\n Update python3 to point to python3.7 with Habana support sudo rm /usr/bin/python3 sudo ln -s /usr/bin/python3.7 /usr/bin/python3  Install Jupyterlab server pip3 install jupyterlab  Jaunch Jupyterlab server and setup tunnel In the current terminal window connected to DL1 instance, run: jupyter notebook --ip='*' --NotebookApp.token='' --NotebookApp.password=''  In a second terminal window on your machine setup port forwarding: ssh -i labsuser.pem -N -L 0.0.0.0:8888:localhost:8888 -L 0.0.0.0:6006:localhost:6006 ubuntu@\u0026lt;IP_ADDRESS\u0026gt;  Open a browser and type localhost:8888/lab  You should see Jupyter lab client on your browser, with the server running on DL1 instance "
},
{
	"uri": "/2_getting_started_dl1.html",
	"title": "2. Deep Learning Training with Habana Models",
	"tags": [],
	"description": "",
	"content": " In this workshop we will showcase the AWS DL1 instance featuring the Habana\u0026reg; Gaudi HPU This session will introduce users to the new instance and show the step-by-step how to start an instance, run Habana’s default models, and convert a public model to run on the instance. We will use Jupyter Notebooks to show these models running on the Instance\nWe will start with the existing models from the Habana Model References and then use a Public Model to show migration steps\n Training instances on EC2 DL1 using models on Habana Github Model-References Repository Migrating models on to EC2 DL1 and training models using TensorFlow Public Model  For simplicity, the workshop is using the full Habana Deep Learning AMI (DLAMI) image from AWS, which contains the Habana drivers, SynpaseAI \u0026reg; Software stack and TensorFlow framework; everything that is needed to run AI workloads.\nAt the end of this workshop, the user will be familiar with how to run models on the DL1 instance and be able to do basic model migration.\n"
},
{
	"uri": "/2_getting_started_dl1/bert.html",
	"title": "2.3 Bert Notebook",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/3_migrate_models_dl1/efficientnet.html",
	"title": "3.3 EfficientNet Notebook",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/4_clean-up/cleanup.html",
	"title": "Delete all resources",
	"tags": [],
	"description": "",
	"content": "This workshop creates the following resources:\nTBD\n"
},
{
	"uri": "/3_migrate_models_dl1.html",
	"title": "3. Model Migration",
	"tags": [],
	"description": "",
	"content": " In this section we will show how to migrate public models over to Habana We\u0026rsquo;ll use two models as examples: 1. A simple MNIST Style model 2. EfficientNet\nFor TensorFlow, Habana integrates the TensorFlow framework with SynapseAI in a plugin using tf.load_library and tf.load_op_library, calling library modules and custom ops/kernels.\nThe framework integration includes three main components: - SynapseAI helpers - Device - Graph passes\nThe TensorFlow framework controls most of the objects required for graph build or graph execution. SynapseAI allows users to create, compile, and launch graph on the device. The Graph passes library optimizes the TensorFlow graph with operations of Pattern Matching, Marking, Segmentation, and Encapsulation (PAMSEN). It is designed to manipulate the TensorFlow graph to fully utilize Gaudi’s HW resources.\nTo prepare your model, you must load the Habana module libraries. To load the Habana Module for TensorFlow, you will call load_habana_module() located under library_loader.py. This function loads the Habana libraries needed to use Gaudi HPU at the TensorFlow level, and these commands can be used:\nimport tensorflow as tf from habana_frameworks.tensorflow import load_habana_module load_habana_module() Once loaded, the Gaudi HPU is registered in TensorFlow and prioritized over CPU. This means that when a given Op is available for both CPU and the Gaudi HPU, the Op is assigned to the Gaudi HPU. When the model is ported to run on the Gaudi HPU, the software stack decides which ops are placed on the CPU and which are placed on the Gaudi HPU. The optimization pass automatically places unsupported ops on the CPU. Ops that do not run on the Gaudi HPU should default to run on the host CPU.\n"
},
{
	"uri": "/3_migrate_models_dl1/next_details_on_customization.html",
	"title": "3.4 More Details on Customization and Model Migration",
	"tags": [],
	"description": "",
	"content": "Next Detail on Customization and Model Migration\n"
},
{
	"uri": "/appendix/resources.html",
	"title": "Technical papers",
	"tags": [],
	"description": "",
	"content": " 1. Whitepaper on AI Fairness https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf 2. Paper on Debugging ML models https://www.amazon.science/publications/amazon-sagemaker-debugger-a-system-for-real-time-insights-into-machine-learning-model-training "
},
{
	"uri": "/1_getting_started/download_workshop_content.html",
	"title": "1.4 Download workshop content",
	"tags": [],
	"description": "",
	"content": " Feel free to follow along with the presenter on the stage or on livestream\n "
},
{
	"uri": "/3_migrate_models_dl1/summary_next_steps.html",
	"title": "3.5 Summary and Next Steps",
	"tags": [],
	"description": "",
	"content": "Users should continuing to run Models from the Habana Model References or brining your own model to DL1 instance; users can refer to additional collateral and videos to continue\nHabana\u0026rsquo;s collateral - Habana Videos - Developer Website - Documentation - Model Repository\nHabana\u0026rsquo;s content in the AWS Marketplace and Elastic Container Registry - AWS Marketplace - Base AMI images with Habana SynaspeAI \u0026reg; Software Stack - ECR Gallery - for TensorFlow and PyTorch Images to pair with Base AMI images\nFor additional support, users can go to Habana\u0026rsquo;s User Forum to post questions and chat with other users\n"
},
{
	"uri": "/appendix/blogposts_videos.html",
	"title": "Blogposts and videos",
	"tags": [],
	"description": "",
	"content": " 1. ML blog posts https://medium.com/@shashankprasanna 2. AWS Blog posts https://aws.amazon.com/blogs/machine-learning/ 3. Habana Developer Page https://developer.habana.ai 4. Habana Videos https://videos.habana.ai "
},
{
	"uri": "/4_clean-up.html",
	"title": "3. Clean up resources",
	"tags": [],
	"description": "",
	"content": "In this section you\u0026rsquo;ll find instructions to clean up resources used for this workshop.\n"
},
{
	"uri": "/appendix.html",
	"title": "Appendix",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]