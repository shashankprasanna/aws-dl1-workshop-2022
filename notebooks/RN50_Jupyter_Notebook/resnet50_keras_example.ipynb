{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd029303-e4b4-4ab6-a868-eaae9d1f1891",
   "metadata": {},
   "source": [
    "# TensorFlow Keras ResNet50 Example on Habana Gaudi<sup>TM</sup>\n",
    "\n",
    "This Jupyter Notebook example demonstrates how to train Keras ResNet50 on Habana Gaudi<sup>TM</sup> device with TensorFlow framework. The neural network is built with Keras APIs, and trained with synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f878d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcff2f3",
   "metadata": {},
   "source": [
    "We will clone Habana `Model-References` repository 0.15.4 branch to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a8b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -b 0.15.4 https://github.com/HabanaAI/Model-References.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a347c854",
   "metadata": {},
   "source": [
    "Check if `Model-References` folder shows up in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957d246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a41ac",
   "metadata": {},
   "source": [
    "Now let's check if `Model-References` repository location is in the sys.path. If not, add it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a09fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a18b77",
   "metadata": {},
   "source": [
    "Add `Model-References` location and `resnet_keras` location to the `sys.path` so that the dependent Python packages in `Model-References` repository are loaded for ResNet50 training. \n",
    "\n",
    "**NOTE:** The following Python statements assume the `Model-Reference` repository was cloned to $HOME directory. If you clone it to a different location, modify it accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f471bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/ubuntu/Model-References/TensorFlow/computer_vision/Resnets/resnet_keras')\n",
    "sys.path.append('/home/ubuntu/Model-References')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba758009-a4ff-4afd-9a74-fceb7a566436",
   "metadata": {},
   "source": [
    "Now, let's import the common Python packages and tensorflow framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-europe",
   "metadata": {},
   "source": [
    "Then, we will import the dependent Python packages from Habana Model-References repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TensorFlow.common.modeling import performance\n",
    "from TensorFlow.common.training import controller\n",
    "from TensorFlow.common.library_loader import load_habana_module\n",
    "from TensorFlow.common.debug import dump_callback\n",
    "from TensorFlow.common.horovod_helpers import synapse_logger_init\n",
    "from TensorFlow.common.tb_utils import write_hparams_v2\n",
    "\n",
    "from TensorFlow.utils.logs import logger\n",
    "from TensorFlow.utils.misc import distribution_utils\n",
    "from TensorFlow.utils.misc import keras_utils\n",
    "from TensorFlow.utils.misc import model_helpers\n",
    "\n",
    "from TensorFlow.computer_vision.common import imagenet_preprocessing\n",
    "from TensorFlow.computer_vision.Resnets.resnet_keras.local_flags import core as flags_core\n",
    "from TensorFlow.computer_vision.Resnets.utils.optimizers.keras import lars_util\n",
    "from TensorFlow.computer_vision.Resnets.resnet_keras import common\n",
    "from TensorFlow.computer_vision.Resnets.resnet_keras import resnet_runnable\n",
    "from TensorFlow.computer_vision.Resnets.resnet_keras.common import adjust_batch_size\n",
    "\n",
    "from central.habana_model_yaml_config import HabanaModelYamlConfig\n",
    "from central.habana_model_runner_utils import HabanaEnvVariables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-insurance",
   "metadata": {},
   "source": [
    "Now, let's define the arguments for Keras APIs, Habana APIs, and LARS optimizer APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity(logging.INFO)\n",
    "# define keras args\n",
    "common.define_keras_flags()\n",
    "# define habana args\n",
    "common.define_habana_flags()\n",
    "# define LARS args\n",
    "lars_util.define_lars_flags()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-rouge",
   "metadata": {},
   "source": [
    "Then we create a function to parse the arguments from yaml configuration file to get user's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args_yaml_config(config_file):\n",
    "    yaml_config = HabanaModelYamlConfig('resnet_keras', config_file)\n",
    "    \n",
    "    env_args = yaml_config.get_env_vars()\n",
    "    model_params=yaml_config.get_parameters()\n",
    "    \n",
    "    cmd_args = []\n",
    "    exclude_fields = ['use_horovod', 'num_workers_per_hls', 'hls_type']\n",
    "    yaml_config.add_parameters_except(cmd_args, exclude_fields)\n",
    "    \n",
    "    return env_args, cmd_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-failure",
   "metadata": {},
   "source": [
    "Let's define the functions used for ResNet50 training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stats(runnable, time_callback):\n",
    "    \"\"\"Normalizes and returns dictionary of stats.\n",
    "\n",
    "    Args:\n",
    "      runnable: The module containing all the training and evaluation metrics.\n",
    "      time_callback: Time tracking callback instance.\n",
    "\n",
    "    Returns:\n",
    "      Dictionary of normalized results.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "\n",
    "    if not runnable.flags_obj.skip_eval:\n",
    "        stats['eval_loss'] = runnable.test_loss.result().numpy()\n",
    "        stats['eval_acc'] = runnable.test_accuracy.result().numpy()\n",
    "\n",
    "        stats['train_loss'] = runnable.train_loss.result().numpy()\n",
    "        stats['train_acc'] = runnable.train_accuracy.result().numpy()\n",
    "\n",
    "    if time_callback:\n",
    "        timestamp_log = time_callback.timestamp_log\n",
    "        stats['step_timestamp_log'] = timestamp_log\n",
    "        stats['train_finish_time'] = time_callback.train_finish_time\n",
    "        if time_callback.epoch_runtime_log:\n",
    "            stats['avg_exp_per_second'] = time_callback.average_examples_per_second\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_train_iterations(flags_obj):\n",
    "    \"\"\"Returns the number of training steps, train and test epochs.\"\"\"\n",
    "    \n",
    "    train_steps = (\n",
    "        imagenet_preprocessing.NUM_IMAGES['train'] // adjust_batch_size(flags_obj.batch_size))\n",
    "    train_epochs = flags_obj.train_epochs\n",
    "\n",
    "    if flags_obj.train_steps:\n",
    "        train_steps = min(flags_obj.train_steps, train_steps)\n",
    "        train_epochs = 1\n",
    "\n",
    "    eval_steps = (\n",
    "        imagenet_preprocessing.NUM_IMAGES['validation'] // flags_obj.batch_size)\n",
    "\n",
    "    return train_steps, train_epochs, eval_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _steps_to_run(steps_in_current_epoch, steps_per_epoch, steps_per_loop):\n",
    "    \"\"\"Calculates steps to run on device.\"\"\"\n",
    "    \n",
    "    if steps_per_loop <= 0:\n",
    "        raise ValueError('steps_per_loop should be positive integer.')\n",
    "    if steps_per_loop == 1:\n",
    "        return steps_per_loop\n",
    "    return min(steps_per_loop, steps_per_epoch - steps_in_current_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-mauritius",
   "metadata": {},
   "source": [
    "Finally, we will define the function to run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(flags_obj):\n",
    "    \"\"\"Run ResNet50 training with synthetic data and eval loop using custom training loops.\n",
    "\n",
    "    Args:\n",
    "      flags_obj: An object containing parsed flag values.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If fp16 is passed as it is not currently supported.\n",
    "\n",
    "    Returns:\n",
    "      Dictionary of training and eval stats.\n",
    "    \"\"\"\n",
    "    \n",
    "    keras_utils.set_session_config(\n",
    "        enable_eager=flags_obj.enable_eager,\n",
    "        enable_xla=flags_obj.enable_xla)\n",
    "    performance.set_mixed_precision_policy(flags_core.get_tf_dtype(flags_obj))\n",
    "\n",
    "    # set data format\n",
    "    data_format = flags_obj.data_format\n",
    "    \n",
    "    if data_format is None:\n",
    "        data_format = ('channels_first'\n",
    "                       if tf.test.is_built_with_cuda() else 'channels_last')\n",
    "    tf.keras.backend.set_image_data_format(data_format)\n",
    "\n",
    "    batch_size = flags_obj.batch_size\n",
    "    model_dir = flags_obj.model_dir\n",
    "\n",
    "    strategy = distribution_utils.get_distribution_strategy(\n",
    "        distribution_strategy=flags_obj.distribution_strategy,\n",
    "        num_gpus=flags_obj.num_gpus,\n",
    "        all_reduce_alg=flags_obj.all_reduce_alg,\n",
    "        num_packs=flags_obj.num_packs,\n",
    "        tpu_address=flags_obj.tpu)\n",
    "    \n",
    "    train_writer, eval_writer = None, None\n",
    "    if flags_obj.enable_tensorboard:\n",
    "        import os\n",
    "        train_writer = tf.summary.create_file_writer(model_dir)\n",
    "        eval_writer = tf.summary.create_file_writer(os.path.join(model_dir, 'eval'))\n",
    "        write_hparams_v2(train_writer, flags_obj.flag_values_dict())\n",
    "    \n",
    "\n",
    "    per_epoch_steps, train_epochs, eval_steps = get_num_train_iterations(\n",
    "        flags_obj)\n",
    "    steps_per_loop = min(flags_obj.steps_per_loop, per_epoch_steps)\n",
    "    train_steps = train_epochs * per_epoch_steps\n",
    "\n",
    "    logging.info(\n",
    "        'Training %d epochs, each epoch has %d steps, '\n",
    "        'total steps: %d; Eval %d steps', train_epochs, per_epoch_steps,\n",
    "        train_steps, eval_steps)\n",
    "    \n",
    "    time_callback = keras_utils.TimeHistory(\n",
    "        batch_size,\n",
    "        flags_obj.log_steps,\n",
    "        summary_writer=train_writer,\n",
    "        batch_size_per_node=flags_obj.batch_size)\n",
    "    \n",
    "    profiler_callback = None\n",
    "    if flags_obj.profile_steps is not None:\n",
    "        profiler_callback = keras_utils.get_profiler_callback(\n",
    "            model_dir,\n",
    "            flags_obj.profile_steps,\n",
    "            flags_obj.enable_tensorboard,\n",
    "            per_epoch_steps)\n",
    "    with distribution_utils.get_strategy_scope(strategy):\n",
    "        runnable = resnet_runnable.ResnetRunnable(flags_obj, time_callback,\n",
    "                                                  train_steps,\n",
    "                                                  per_epoch_steps,\n",
    "                                                  profiler_callback)\n",
    "\n",
    "    eval_interval = flags_obj.epochs_between_evals * per_epoch_steps\n",
    "    checkpoint_interval = (\n",
    "        per_epoch_steps if flags_obj.enable_checkpoint_and_export else None)\n",
    "    summary_interval = per_epoch_steps if flags_obj.enable_tensorboard else None\n",
    "\n",
    "    checkpoint_manager = tf.train.CheckpointManager(\n",
    "        runnable.checkpoint,\n",
    "        directory=model_dir,\n",
    "        max_to_keep=10,\n",
    "        step_counter=runnable.global_step,\n",
    "        checkpoint_interval=checkpoint_interval)\n",
    "\n",
    "    train_steps=per_epoch_steps * train_epochs\n",
    "\n",
    "    resnet_controller = controller.Controller(\n",
    "        strategy,\n",
    "        runnable.train,\n",
    "        runnable.evaluate,\n",
    "        global_step=runnable.global_step,\n",
    "        steps_per_loop=steps_per_loop,\n",
    "        train_steps=train_steps,\n",
    "        checkpoint_manager=checkpoint_manager,\n",
    "        summary_interval=summary_interval,\n",
    "        eval_steps=eval_steps,\n",
    "        eval_interval=eval_interval)\n",
    "\n",
    "    time_callback.on_train_begin()\n",
    "    resnet_controller.train(evaluate=not flags_obj.skip_eval)\n",
    "    time_callback.on_train_end()\n",
    "\n",
    "    stats = build_stats(runnable, time_callback)\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-resistance",
   "metadata": {},
   "source": [
    "After we defined all the functions needed for ResNet50 training, let's start running the workload.\n",
    "\n",
    "First of all, we will parse `resnet50_keras_lars_bf16_1card.yaml` configuration file to get the input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_args, cmd_args = parse_args_yaml_config('resnet50_keras_lars_bf16_1card.yaml')\n",
    "\n",
    "cmd_args.insert(0, ' ') # workaround cmd line args \n",
    "argv = flags.FLAGS(cmd_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-japan",
   "metadata": {},
   "source": [
    "Then we will initialize preloading libraries and Synapse logger API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0802aac-632c-474a-b312-0ad89ceb2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "common.initialize_preloading()\n",
    "# initialize synapse logger\n",
    "synapse_logger_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed46a62-994c-4c9f-ba70-7870c2a9028f",
   "metadata": {},
   "source": [
    "Load Habana TensorFlow modules and aquire Habana Gaudi device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdafaaee-a3a3-4f0f-a3b3-c26f4aa331dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_info_devices = load_habana_module()\n",
    "logging.info('Devices:\\n%s', log_info_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4446c9-3a32-4a79-814c-b77faee54310",
   "metadata": {},
   "source": [
    "Now, let's launch ResNet50 training with LARS optimizer on HPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d92cc-601b-46b8-8bc4-04fb509a3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flags.FLAGS.global_seed:\n",
    "    tf.random.set_seed(flags.FLAGS.global_seed)\n",
    "\n",
    "with HabanaEnvVariables(env_args):\n",
    "    with dump_callback():\n",
    "        model_helpers.apply_clean(flags.FLAGS)\n",
    "        with logger.benchmark_context(flags.FLAGS):\n",
    "            stats =run(flags.FLAGS)\n",
    "        logging.info('Run stats:\\n%s', stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45de297-cbb4-4547-8f71-269bf41f4e33",
   "metadata": {},
   "source": [
    "To check the training results, we will load TensorBoard extension to this Jupyter Notebook and display the results in charts.\n",
    "\n",
    "**Note**, if you are running Jupyter Notebook on a web browser on your local machine, you need to forward TensorBoard port number (default is 6006) to the Jupyter Notebook server. The command for port forward looks like this: `ssh -i \"dl1-key.pem\" -N -f -L localhost:6006:localhost:6006 ubuntu@ec2-35-173-178-212.compute-1.amazonaws.com`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d2d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d606e",
   "metadata": {},
   "source": [
    "The following command assumes tfevents were dumped to `./model_tmp` directory as specified in `--model_dir` argument in the `resnet50_keras_lars_bf16_1card.yaml` configuration file. Modify it accordingly if you use a different folder as `--model_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --bind_all --logdir model_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21207d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
